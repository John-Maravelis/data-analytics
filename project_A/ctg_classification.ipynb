{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ctg_classification.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1md3eBL58rwqiuGy5fstml6HDvIqc3NX_","authorship_tag":"ABX9TyN3GhWU+wh68avAMmxlBHkT"},"kernelspec":{"name":"python3","display_name":"Python 3.7.9 64-bit"},"language_info":{"name":"python","version":"3.7.9"},"interpreter":{"hash":"35e3af59d3e06b5bab0f409a858e458c29f9e188770119e2e5e93795b5d1e9ba"}},"cells":[{"cell_type":"code","metadata":{"id":"0gfBejFbtsmg","executionInfo":{"status":"ok","timestamp":1624041399310,"user_tz":-180,"elapsed":270,"user":{"displayName":"John Maravelis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigRfEbJGXWsactYq0nISE-Xw05ZqETSj2Qpzx5LA=s64","userId":"16274901106749010920"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import matthews_corrcoef, confusion_matrix\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.utils import np_utils\n","from keras.models import Sequential\n","from keras.layers import Dense"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g2fOzyeFuXdf"},"source":["path = './ctg_data_full.xls'\n","# read the ctg data\n","df = pd.read_excel(path, sheet_name='Data', usecols='K:AE', skiprows=0, header=1, nrows=2126)\n","df.dropna(inplace=True) # removing missing values\n","X = df.to_numpy()\n","print(df.head(5))\n","\n","# read the results column. \n","# For the 3-class NSP column use 'AT', for the 10-class FHR use 'AR'.\n","# Change the number of output nodes in the model accordingly.\n","y_df = pd.read_excel(path, sheet_name='Data', usecols='AR', skiprows=0, header=1, nrows=2126)\n","y_df.dropna(inplace=True) # removing missing values\n","y = y_df.to_numpy()\n","print(y_df.head(5))\n","\n","\n","### Scale the dataset\n","\n","# Standardizing\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# # Normalizing, though better results are achieved with standardizing\n","# scaler = MinMaxScaler()\n","# X = scaler.fit_transform(X)\n","\n","# create 0-indexed categories of the result classes\n","encoder = LabelEncoder()\n","y_enc = encoder.fit_transform(y)\n","y_bin = np_utils.to_categorical(y_enc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n1-CRrOXubIg"},"source":["# split data into training: 60%, validation: 10%  and testing: 30%\n","len_data = X.shape[0]\n","train_size = int(len_data * .6)\n","val_size = int(len_data * .1)\n","test_size = len_data - (train_size + val_size)\n","\n","print('Training size:', train_size)\n","print('\\nValidation size:', val_size)\n","print('\\nTesting size:', test_size) \n","\n","# train data\n","x_train = X[:train_size,:] \n","y_train = y_bin[:train_size,:]\n","y_train_true = y[:train_size]\n","\n","# validation data\n","x_val = X[train_size:train_size + val_size,:]\n","y_val = y_bin[train_size:train_size + val_size,:]\n","y_val_true = y[train_size:train_size + val_size]\n","\n","# test data\n","x_test = X[train_size + val_size:,:]\n","y_test = y_bin[train_size + val_size:,:]\n","y_test_true = y[train_size + val_size:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_kEHwkHudAd"},"source":["# define the model\n","model = Sequential()\n","\n","# add layers: input, 2 hidden and an output\n","model.add(Dense(units=50, input_dim=21, activation='relu'))\n","model.add(Dense(units=30, activation='relu'))\n","model.add(Dense(units=30, activation='relu'))\n","model.add(Dense(units=10, activation='softmax'))\n","print(model.summary())\n","\n","# compile the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n","\n","# train the model\n","history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=30, batch_size=1, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDQmSL3uuery"},"source":["### Accuracy calculation\n","\n","# evaluate the training \n","score = model.evaluate(x_train, y_train, batch_size=1, verbose=0)\n","print(f'Train { model.metrics_names[1] }: { round(score[1]*100, 2) }%')\n","\n","# train, validation and test predictions accuracy and reverting the catigorization with np.argmax\n","y_train_pred = model.predict(x_train, batch_size=1)\n","y_train_pred = np.argmax(y_train_pred, axis=1)\n","temp = 100*sum(y_train_true == y_train_pred)/y_train.shape[0]\n","train_acc = float(round(sum(temp)/temp.shape[0], 2))\n","print(f'\\nTrain Accuracy by model.predict: { train_acc }%')\n","\n","y_val_pred = model.predict(x_val, batch_size=1)\n","y_val_pred = np.argmax(y_val_pred, axis=1)\n","temp = 100*sum(y_val_true == y_val_pred)/y_val.shape[0]\n","val_acc = float(round(sum(temp)/temp.shape[0], 2))\n","print(f'\\nValidation Accuracy by model.predict: { val_acc }%')\n","\n","y_test_pred = model.predict(x_test, batch_size=1)\n","y_test_pred = np.argmax(y_test_pred, axis=1)\n","temp = 100*sum(y_test_true == y_test_pred)/y_test.shape[0]\n","test_acc = float(round(sum(temp)/temp.shape[0], 2))\n","print(f'\\nTest Accuracy by model.predict: { test_acc }%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vn9V-7dG8Uo"},"source":["### Metrics\n","\n","# Matthews correlation coefficient\n","print('Train MCC:', matthews_corrcoef(y_train_true, y_train_pred))\n","print('\\nVal MCC:', matthews_corrcoef(y_val_true, y_val_pred))\n","print('\\nTest MCC:', matthews_corrcoef(y_test_true, y_test_pred))\n","\n","# confusion matrix\n","print('\\n====================TRAIN====================')\n","print(confusion_matrix(y_train_true, y_train_pred))\n","print('\\n====================VAL====================')\n","print(confusion_matrix(y_val_true, y_val_pred))\n","print('\\n====================TEST====================')\n","print(confusion_matrix(y_test_true, y_test_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9rwKFGWWudm4"},"source":["### history loss function\n","\n","# Plot training and validation loss\n","plt.figure(figsize=(20, 10))\n","plt.subplot(2, 1, 1)\n","plt.title('Model loss')\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.plot(history.history['loss'], c='r')\n","plt.plot(history.history['val_loss'], c='g')\n","plt.legend(['train', 'validation'], loc='upper right')\n","plt.subplot(2, 1, 2)\n","plt.title('Model accuracy')\n","plt.xlabel('epochs')\n","plt.ylabel('acccuracy')\n","plt.plot(history.history['accuracy'], c='r')\n","plt.plot(history.history['val_accuracy'], c='g')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]}]}