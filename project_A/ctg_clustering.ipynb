{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd24b129",
   "metadata": {
    "id": "cd24b129"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, KBinsDiscretizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07273759",
   "metadata": {
    "id": "07273759"
   },
   "outputs": [],
   "source": [
    "path = './ctg_data_full.xls'\n",
    "# read the ctg data\n",
    "df = pd.read_excel(path, sheet_name='Data', usecols='K:AE', skiprows=0, header=1, nrows=2126)\n",
    "df.dropna(inplace=True)\n",
    "X = df.to_numpy()\n",
    "print(df.head(5))\n",
    "\n",
    "# read the results column. \n",
    "# For the 3-class NSP column use 'AT', for the 10-class FHR use 'AR'.\n",
    "df_y = pd.read_excel(path, sheet_name='Data', usecols='AR', skiprows=0, header=1, nrows=2126)\n",
    "df_y.dropna(inplace=True)\n",
    "y = df_y.to_numpy()\n",
    "print(df_y.head(5))\n",
    "'''\n",
    "NOTE: Comment out whichever part you want to use. Choose no more than one from every category.\n",
    "'''\n",
    "\n",
    "### Data scaling\n",
    "\n",
    "# Standardizing\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# # Normalizing, though better results are achieved with standardizing\n",
    "# scaler = MinMaxScaler()\n",
    "# X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZQvRDIdIolk6",
   "metadata": {
    "id": "ZQvRDIdIolk6"
   },
   "outputs": [],
   "source": [
    "### Data Discetization\n",
    "\n",
    "# # K-Means Discretization\n",
    "# discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans')\n",
    "# X = discretizer.fit_transform(X)\n",
    "\n",
    "# # Equal-Frequency Discretization\n",
    "# discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "# X = discretizer.fit_transform(X)\n",
    "\n",
    "# # Equal-Width Discretization\n",
    "# discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "# X = discretizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EE7WkVeIojZh",
   "metadata": {
    "id": "EE7WkVeIojZh"
   },
   "outputs": [],
   "source": [
    "### Dimensionality reduction \n",
    "\n",
    "# # Principal Component Analysis\n",
    "# pca = PCA(n_components=17)\n",
    "# X = pca.fit_transform(X)\n",
    "\n",
    "# # Linear Discriminant Analysis\n",
    "# lda = LDA(n_components=17)\n",
    "# X = lda.fit_transform(X)\n",
    "\n",
    "# # Singular Value Decomposition\n",
    "# svd = TruncatedSVD(n_components=17)\n",
    "# X = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sUroPCaZhvLq",
   "metadata": {
    "id": "sUroPCaZhvLq"
   },
   "outputs": [],
   "source": [
    "### Clustering techniques\n",
    "\n",
    "# K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=15).fit(X)\n",
    "labels = kmeans.labels_\n",
    "# NOTE: Different random_state values were used with 15 having the best performance\n",
    "\n",
    "# # Agglomerative clustering\n",
    "# agglomerative = AgglomerativeClustering(n_clusters=3).fit(X)\n",
    "# labels = agglomerative.labels_\n",
    "# # NOTE: The best performing linkage was the default one (ward)\n",
    "\n",
    "# # DBSCAN clustering\n",
    "# dbscan = DBSCAN(eps=2, min_samples=12).fit(X)\n",
    "# labels = dbscan.labels_\n",
    "# # # NOTE: The only time I managed to get to 3 (2 plus the noise) classes was with: eps=2, min_samples=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1elH1L9sMA_n",
   "metadata": {
    "id": "1elH1L9sMA_n"
   },
   "outputs": [],
   "source": [
    "# count the occurences of the different original clusters \n",
    "unique, count = np.unique(y, return_counts=True)\n",
    "true_results = np.asarray((unique, count)).T\n",
    "\n",
    "# count the occurences of the different predicted clusters \n",
    "unique, count = np.unique(labels, return_counts=True)\n",
    "predicted_results = np.asarray((unique, count)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reverse the scaling process\n",
    "\n",
    "# X = pca.inverse_transform(X)\n",
    "# X = discretizer.inverse_transform(X)\n",
    "X = scaler.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EwgrPRw0KJ8g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 823
    },
    "executionInfo": {
     "elapsed": 1676,
     "status": "ok",
     "timestamp": 1623776564078,
     "user": {
      "displayName": "John Maravelis",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GigRfEbJGXWsactYq0nISE-Xw05ZqETSj2Qpzx5LA=s64",
      "userId": "16274901106749010920"
     },
     "user_tz": -180
    },
    "id": "EwgrPRw0KJ8g",
    "outputId": "24454c3d-f0d3-4cce-afd2-1aa236c098b9"
   },
   "outputs": [],
   "source": [
    "### Plots\n",
    "\n",
    "# plot the original clusters on the unscaled data set\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(X[:,0], X[:,1], c=y)\n",
    "legend = ax.legend(*scatter.legend_elements(), title='Classes', \n",
    "                        bbox_to_anchor=(1.05, 1.0), loc='upper left') # move the legend outside the graph\n",
    "ax.add_artist(legend)\n",
    "plt.title('Original')\n",
    "plt.show()\n",
    "\n",
    "# plot the predicted clusters from K-MEANS on the unscaled data set\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(X[:,0], X[:,1], c=labels)\n",
    "legend = ax.legend(*scatter.legend_elements(), title='Classes', \n",
    "                        bbox_to_anchor=(1.05, 1.0), loc='upper left') # move the legend outside the graph\n",
    "ax.add_artist(legend)\n",
    "plt.title('Clustering Algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91da10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1623776565569,
     "user": {
      "displayName": "John Maravelis",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GigRfEbJGXWsactYq0nISE-Xw05ZqETSj2Qpzx5LA=s64",
      "userId": "16274901106749010920"
     },
     "user_tz": -180
    },
    "id": "6e91da10",
    "outputId": "e310f439-1fe5-4df7-81b0-30e3e9267c8b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Metrics\n",
    "\n",
    "# confusion matrix\n",
    "print('Confusion Matrix:\\n {}'.format(confusion_matrix(true_results[:,1], predicted_results[:,1])))\n",
    "\n",
    "# F1-score\n",
    "print('\\nF1: {}'.format(f1_score(true_results[:,1], predicted_results[:,1], average='micro')))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ctg_clustering.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "35e3af59d3e06b5bab0f409a858e458c29f9e188770119e2e5e93795b5d1e9ba"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}